\subsection{Extended Kalman Filter}
The model is , for $t = 1, \dotsc, T$:
\begin{align}
	\vec x_t	&= \vec g(\vec x_{t - 1}, \vec u_t) + \vec \epsilon_t 	&\in \mathbb R^n\\
	\vec z_t	&= \vec h(\vec x_t, \vec u_t) + \vec \delta_t 			&\in \mathbb R^m\\
	\vec\epsilon_t	&\sim \Gauss(\vec 0, \vec Q_t) 						&\in \mathbb R^n\\
	\vec\delta_t	&\sim \Gauss(\vec 0, \vec R_t) 						&\in \mathbb R^m
\end{align}

The posteriors of interest are (we drop the conditional dependence on $\vec \theta_t$'s):
\begin{align}
	p(\vec x_t \mid \vec z_{1:t - 1}, \vec u_{1:t})	&\approx \Gauss\left(\vec x_t \mid \vec \mu_{t \mid t - 1}, \vec\Sigma_{t \mid t - 1}\right) \\
	\vec \mu_{t \mid t - 1}							&\approx \vec g(\vec \mu_{t - 1 \mid t - 1}, \vec u_t) \\
	\vec \Sigma_{t \mid t - 1}						&\approx \vec G_t \vec \Sigma_{t - 1 \mid t - 1} \vec G_t^T + \vec Q_t \\
	p(\vec x_t \mid \vec z_{1:t}, \vec u_{1:t}) 	&\approx\ \Gauss\left(\vec x_t \mid \vec \mu_{t \mid t}, \vec\Sigma_{t \mid t}\right) \\
	\vec \mu_{t \mid t}								&\approx \vec \mu_{t \mid t - 1} + \vec W_t \left(\vec z_t - \vec h(\vec \mu_{t \mid t - 1}, \vec u_t)\right) \\
	\vec \Sigma_{t \mid t}							&\approx \vec \Sigma_{t \mid t - 1} - \vec W_t \vec S_t \vec W_t^T
\end{align}

Note that we make two types of approximations: (1) we assume the posteriors are Gaussians (which they are not) and (2) we calculate the moments using linearised versions of random variables of interest. We define previously undefined variables below.

\subsubsection{Derivations}
The derivation of the prediction equations is as follows:
\begin{align}
	\vec \mu_{t \mid t - 1}	&= \E\left[\vec x_t \mid \vec z_{1:t - 1}, \vec u_{1:t}\right] \\
							&= \E\left[\vec g(\vec x_{t - 1}, \vec u_t) + \vec \epsilon_t \mid \vec z_{1:t - 1}, \vec u_{1:t}\right] \\
							&= \E\left[\vec g(\vec \mu_{t - 1 \mid t - 1}, \vec u_t) + \vec G_t (\vec x_{t - 1} - \vec \mu_{t - 1 \mid t - 1}) + \cdots + \vec \epsilon_t \mid \vec z_{1:t - 1}, \vec u_{1:t}\right]
\end{align}
We have linearised around $\vec \mu_{t - 1 \mid t - 1}$ where $\vec G_t$ is the Jacobian of $\vec g(\vec x, \vec u)$ w.r.t. $\vec x$ evaluated at $\vec \mu_{t - 1 \mid t - 1}$:
\begin{align}
	\vec G_t	&\triangleq \frac{\partial \vec g(\vec x, \vec u)}{\partial \vec x}\at[\bigg]{\vec x = \vec \mu_{t - 1 \mid t - 1}, \vec u = \vec u_t}
\end{align}
i.e. $\frac{\partial \vec g(\vec x, \vec u)}{\partial \vec x} \in \mathbb R^{n \times n}$ and the $(i, j)^{\text{th}}$ element is $\frac{\partial g_i(\vec x, \vec u)}{\partial x_j}$. Taking only the first two terms of the Taylor expansion and continuing with the derivation:
\begin{align}
	\vec \mu_{t \mid t - 1}	&\approx \E\left[\vec g(\vec \mu_{t - 1 \mid t - 1}, \vec u_t) + \vec G_t (\vec x_{t - 1} - \vec \mu_{t - 1 \mid t - 1}) + \vec \epsilon_t \mid \vec z_{1:t - 1}, \vec u_{1:t}\right] \\
							&\,\,\,\vdots \\
							&= \vec g(\vec \mu_{t - 1 \mid t - 1}, \vec u_t)
\end{align}
Similarly, the prediction equation for the covariance can be derived as follows:
\begin{align}
	\vec \Sigma_{t \mid t - 1}	&= \var\left[\vec x_t \mid \vec z_{1:t - 1}, \vec u_{1:t}\right] \\
								&= \var\left[\vec g(\vec x_{t - 1}, \vec u_t) + \vec \epsilon_t \mid \vec z_{1:t - 1}, \vec u_{1:t}\right] \\
								&= \var\left[\vec g(\vec \mu_{t - 1 \mid t - 1}, \vec u_t) + \vec G_t (\vec x_{t - 1} - \vec \mu_{t - 1 \mid t - 1}) + \cdots + \vec \epsilon_t \mid \vec z_{1:t - 1}, \vec u_{1:t}\right] \\
								&\approx \var\left[\vec g(\vec \mu_{t - 1 \mid t - 1}, \vec u_t) + \vec G_t (\vec x_{t - 1} - \vec \mu_{t - 1 \mid t - 1}) + \vec \epsilon_t \mid \vec z_{1:t - 1}, \vec u_{1:t}\right] \\
								&\,\,\,\vdots \\
								&= \vec G_t \vec \Sigma_{t - 1 \mid t - 1} \vec G_t^T + \vec Q_t
\end{align}

The derivation of the update equations is as follows:
\begin{align}
	\vec \mu_{t \mid t}	&= \E\left[\vec x_t \mid \vec z_{1:t}, \vec u_{1:t}\right] \\
						&\,\,\,\vdots \\
						&= \vec \mu_{t \mid t - 1} + \vec W_t \left(\vec z_t - \vec h(\vec \mu_{t \mid t - 1}, \vec u_t)\right) \\
	\vec \Sigma_{t \mid t}	&= \var\left[\vec x_t \mid \vec z_{1:t}, \vec u_{1:t}\right] \\
							&= \E\left[(\vec \mu_{t \mid t} - \vec x_t)^T (\vec \mu_{t \mid t} - \vec x_t) \mid \vec z_{1:t}, \vec u_{1:t}\right] \\
							&\,\,\,\vdots \\
							&= \vec \Sigma_{t \mid t - 1} - \vec W_t \vec S_t \vec W_t^T
\end{align}
where
\begin{align}
	\vec W_t &= \vec \Sigma_{t | t - 1} \vec H_t \vec S_t \\
	\vec S_t &= \vec H_t \vec \Sigma_{t | t - 1} \vec H_t + \vec R_t
\end{align}

We have linearised around $\vec \mu_{t \mid t - 1}$ where $\vec H_t$ is the Jacobian of $\vec h(\vec x, \vec u)$ w.r.t. $\vec x$ evaluated at $\vec \mu_{t \mid t - 1}$:
\begin{align}
	\vec H_t	&\triangleq \frac{\partial \vec h(\vec x, \vec u)}{\partial \vec x}\at[\bigg]{\vec x = \vec \mu_{t \mid t - 1}, \vec u = \vec u_t}
\end{align}
i.e. $\frac{\partial \vec h(\vec x, \vec u)}{\partial \vec x} \in \mathbb R^{m \times n}$ and the $(i, j)^{\text{th}}$ element is $\frac{\partial h_i(\vec x, \vec u)}{\partial x_j}$.