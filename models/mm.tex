\section{Mixture models}
In mixture models, we have discrete latent states $\vec Z = \left\{z_n, z_n \in \{1, \dotsc, K\}\right\}, n = 1, \dotsc, N$ and observed states $\vec X = \left\{\vec x_n, \vec x_n \in \mathbb R^D\right\}, n = 1, \dotsc, N$. We set the priors and the class conditional likelihoods to be $p(z_n) = \Cat(\vec \pi), \vec \pi = (\pi_1, \dotsc, \pi_K)$ and $p(\vec x_n \mid z_n = k; \vec \theta) = p_k(\vec x_n \mid \vec \theta)$. We can thus express the likelihood of the observed variables to be:
\begin{align}
    p(\vec x_n \mid \vec \theta)    &= \sum_{k = 1}^K p(\vec x_n, z_n = k; \vec \theta) \nonumber\\
                                    &= \sum_{k = 1}^K p(\vec x_n \mid z_n = k; \vec \theta) p(z_n = k \mid \vec \theta) \nonumber\\
                                    &= \sum_{k = 1}^K \pi_k p_k(\vec x_n \mid \vec \theta) \label{eqn:models/mm/lik}
\end{align}

We can also express the posterior probability that point $n$ belongs to cluster $k$, or the \emph{responsibility} $r_{nk}(\vec \theta)$ (often abbreviated as $r_{nk}$) of cluster $k$ for point $n$ to be:
\begin{align}
    r_{nk}(\vec \theta) &\triangleq p(z_n = k \mid \vec x_n; \vec \theta) \nonumber\\
                        &= \frac{p(\vec x_n \mid z_n = k; \vec \theta) p(z_n = k \mid \vec \theta)}{\sum_{k' = 1}^K p(\vec x_n \mid z_n = k'; \vec \theta) p(z_n = k' \mid \vec \theta)} \label{eqn:models-mm-responsibility}
\end{align}

Evaluating the above is called \emph{soft clustering}. \emph{Hard clustering} finds the MAP estimate as follows:
\begin{align}
    z_n^\ast    &= \argmax_k r_{nk} \nonumber\\
                &= \argmax_k \left\{\log p(\vec x_n \mid z_n = k; \vec \theta) + \log(z_n = k \mid \vec \theta)\right\}
\end{align}

\emph{Unidentifiability} refers to the fact that the posterior distribution for the parameter $p(\vec \theta \mid \mathcal D)$ can be multimodal (with equal peaks) and hence cant find a unique ML/MAP estimate.

We distinguish between two log likelihoods -- log likelihood for the observed data, denoted by $\ell(\vec \theta)$ and log likelihood for complete data, denoted by $\ell_c(\vec \theta)$. These two quantities can be expressed as:
\begin{align}
    \ell(\vec \theta)   &\triangleq \log p(\mathcal D \mid \vec \theta) \nonumber\\
                        &= \log \prod_{n = 1}^N p(\vec x_n \mid \vec \theta)\nonumber\\
                        &= \log \left\{\prod_{n = 1}^N \sum_{k = 1}^K p(\vec x_n, z_n = k \mid \vec \theta)\right\} \nonumber\\
                        &= \sum_{n = 1}^N \log \sum_{k = 1}^K p(\vec x_n, z_n = k \mid \vec \theta)\\
    \ell_c(\vec \theta) &\triangleq \log p\left(\{\vec x_n, z_n\} \mid \vec \theta\right) \nonumber\\
                        &= \log \prod_n p(\vec x_n, z_n \mid \vec \theta) \nonumber\\
                        &= \sum_n \log p(\vec x_n, z_n \mid \vec \theta)
\end{align}
The log likelihood for observed data, $\ell(\vec \theta)$ can't be guaranteed to be convex so it might be intractable to find ML/MAP estimates. Alternatively, we just express these terms as $\ell(\vec \theta) = \log p(\vec X \mid \vec \theta)$ and $\ell_c(\vec \theta) = \log p (\vec X, \vec Z \mid \vec \theta)$.

\subsection{EM algorithm}
\subsubsection{Maximise the likelihood}
Goal is to maximise 
$$p(\vec X \mid \vec \theta)$$
Assume it's easy to maximise the \emph{auxiliary function}
\begin{align}
    \mathcal Q\left(\vec \theta, \vec \theta^{\text{old}}\right)    &\triangleq \E_{\vec Z \,\sim \,\cdot \mid \vec X; \vec \theta^{\text{old}}} \left[\ell_c(\vec \theta)\right]
\end{align}
w.r.t. $\vec \theta$. Note that this function can be rewritten as either
\begin{align}
    \mathcal Q\left(\vec \theta, \vec \theta^{\text{old}}\right)    &= \sum_{\vec Z} p\left(\vec Z \mid \vec X; \vec \theta^{\text{old}}\right) \ln p\left(\vec X, \vec Z \mid \vec \theta\right)
\end{align}
or
\begin{align}
    \mathcal Q\left(\vec \theta, \vec \theta^{\text{old}}\right)    &= \E_{\vec Z \,\sim \,\cdot \mid \vec X; \vec \theta^{\text{old}}} \left[\ln p\left(\vec X, \vec Z \mid \vec \theta\right)\right] \\
                                                                    &= \E_{\vec Z \,\sim \,\cdot \mid \vec X; \vec \theta^{\text{old}}} \left[\sum_n \ln p\left(\vec x_n, z_n \mid \vec \theta\right)\right] \\
                                                                    &= \sum_n \E_{z_n \,\sim \,\cdot \mid \vec x_n; \vec \theta^{\text{old}}} \left[\ln p(\vec x_n, z_n \mid \vec \theta)\right] \\
                                                                    &= \sum_n \sum_k p\left(z_n = k \mid \vec x_n; \vec \theta^{\text{old}}\right) \ln p(\vec x_n, z_n = k \mid \vec \theta) \\
                                                                    &= \sum_n \sum_k r_{nk}\left(\vec \theta^{\text{old}}\right) \ln \left(\pi_k p(\vec x_n \mid z_n = k; \vec \theta)\right) \\
                                                                    &= \sum_n \sum_k r_{nk}\left(\vec \theta^{\text{old}}\right) \left(\ln \pi_k + \ln p(\vec x_n \mid z_n = k; \vec \theta)\right)
\end{align}

We can express $\ln p(\vec X \mid \vec \theta)$ as
\begin{equation}
    \ln p(\vec X \mid \vec \theta) = \mathcal L(q, \vec \theta) + \KL(q \parallel p) \label{eqn:models/mm/lik2}
\end{equation}
where
\begin{align}
    \mathcal L(q, \vec \theta)  &= \sum_{\vec Z} q(\vec Z) \ln \frac{p(\vec X, \vec Z \mid \vec \theta)}{q(\vec Z)} \\
    \KL(q \parallel p)          &= -\sum_{\vec Z} q(\vec Z) \ln \frac{p(\vec Z \mid \vec X, \vec \theta)}{q(\vec Z)}
\end{align}
because
\begin{align*}
    \text{RHS}  &= \mathcal L(q, \vec \theta) + \KL(q \parallel p)\\
                &= \sum_{\vec Z} q(\vec Z) \ln \frac{p(\vec X, \vec Z \mid \vec \theta)}{q(\vec Z)} - \sum_{\vec Z} q(\vec Z) \ln \frac{p(\vec Z \mid \vec X, \vec \theta)}{q(\vec Z)} \\
                &= \sum_{\vec Z} q(\vec Z) \ln \frac{p(\vec X, \vec Z \mid \vec \theta)}{p(\vec Z \mid \vec X; \vec \theta)} \\
                &= \sum_{\vec Z} q(\vec Z) \ln p(\vec X \mid \vec \theta) \\
                &= \ln p(\vec X \mid \vec \theta) \\
                &= \text{LHS}
\end{align*}

The actual algorithm is as follows
\begin{algorithmbis}[EM algorithm for maximising the likelihood]\label{alg:em}
    \begin{algorithmic}[1]
        \State Initialise $\vec \theta^{\text{new}}$.
        \Repeat
            \State $\vec \theta^{\text{old}} \gets \vec \theta^{\text{new}}$
            \State E step: Set $q(\vec Z) = p\left(\vec Z \mid \vec X, \vec \theta^{\text{old}}\right)$.
            \State M step: Hold $q(\vec Z)$ fixed and set $\vec \theta^{\text{new}} = \argmax_{\vec \theta} \mathcal Q\left(\vec \theta, \vec \theta^{\text{old}}\right)$.
        \Until{convergence.}
    \end{algorithmic}
\end{algorithmbis}

\paragraph{E step.} Hold $\vec \theta^{\text{old}}$, maximise $\mathcal L\left(q, \vec \theta^{\text{old}}\right)$ w.r.t. $q$. Since the quantity $\ln p(\vec X \mid \vec \theta)$ in \eqref{eqn:models/mm/lik2} is constant w.r.t. $q$, we can maximise $\mathcal L\left(q, \vec \theta^{\text{old}}\right)$ by minimising $\KL(q \parallel p)$. This can be done by setting the $\KL$ to $0$ by setting $q(\vec Z) = p(\vec Z \mid \vec X; \vec \theta^{\text{old}})$.

\paragraph{M step.} Hold $q(\vec Z) = p(\vec Z \mid \vec X; \vec \theta^{\text{old}})$ fixed, maximise $\mathcal L\left(q, \vec \theta\right)$ w.r.t. $\vec \theta$ to get $\vec \theta^{\text{new}}$. We can rewrite $\mathcal L\left(q, \vec \theta\right)$ as
\begin{align*}
    \mathcal L\left(q, \vec \theta\right)   &= \sum_{\vec Z} q(\vec Z) \ln \frac{p(\vec X, \vec Z \mid \vec \theta)}{q(\vec Z)} \\
                                            &= \sum_{\vec Z} q(\vec Z) \ln p\left(\vec X, \vec Z \mid \vec \theta\right) - \sum_{\vec Z} q(\vec Z) \ln q(\vec Z) \\
                                            &= \sum_{\vec Z} p\left(\vec Z \mid \vec X; \vec \theta^{\text{old}}\right) \ln p\left(\vec X, \vec Z \mid \vec \theta\right) - \sum_{\vec Z} p\left(\vec Z \mid \vec X; \vec \theta^{\text{old}}\right) \ln p\left(\vec Z \mid \vec X; \vec \theta^{\text{old}}\right) \\
                                            &= \mathcal Q\left(\vec \theta, \vec \theta^{\text{old}}\right) + \text{constant w.r.t. } \vec \theta
\end{align*}
from which we can see that we should maximise $\mathcal Q\left(\vec \theta, \vec \theta^{\text{old}}\right)$. In both steps, the value of $\mathcal L(q, \vec \theta)$ increases.

\subsubsection{Maximising the posterior}
Goal is to maximise
\begin{equation*}
    p(\vec \theta \mid \vec X)
\end{equation*}
Assume it's easy to maximise
\begin{equation}
    \mathcal Q\left(\vec \theta, \vec \theta^{\text{old}}\right) + \ln p(\vec \theta)
\end{equation}
w.r.t. $\vec \theta$.

We can express $\ln p(\vec \theta \mid \vec X)$ as
\begin{align}
    \ln p(\vec \theta \mid \vec X)  &= \ln p(\vec X \mid \vec \theta) + \ln p(\vec \theta) - \ln p(\vec X) \nonumber\\
                                    &= \mathcal L(q, \vec \theta) + \KL(q \parallel p) + \ln p(\vec \theta) - \ln p(\vec X)
\end{align}

\paragraph{E step.} Here, we perform the same thing as in maximising the likelihood, with the same reasons.
\paragraph{M step.} Hold $q(\vec Z) = p(\vec Z \mid \vec X; \vec \theta^{\text{old}})$ fixed, maximise $\mathcal L\left(q, \vec \theta\right) + \ln p(\vec \theta)$ w.r.t. $\vec \theta$ to get $\vec \theta^{\text{new}}$. We can rewrite $\mathcal L\left(q, \vec \theta\right) + \ln p(\vec \theta)$ as
\begin{align*}
    \mathcal L\left(q, \vec \theta\right) + \ln p(\vec \theta)  &= \mathcal Q\left(\vec \theta, \vec \theta^{\text{old}}\right) + \ln p(\vec \theta) + \text{constant w.r.t. } \vec \theta
\end{align*}
from which we can see that we should maximise $\mathcal Q\left(\vec \theta, \vec \theta^{\text{old}}\right) + \ln p(\vec \theta)$. In both steps, the value of $\mathcal L(q, \vec \theta) + \ln p(\vec \theta)$ increases.

The actual algorithm is as follows
\begin{algorithmbis}[EM algorithm for maximising the posterior]\label{alg:em-post}
    \begin{algorithmic}[1]
        \State Initialise $\vec \theta^{\text{new}}$.
        \Repeat
            \State $\vec \theta^{\text{old}} \gets \vec \theta^{\text{new}}$
            \State E step: Set $q(\vec Z) = p\left(\vec Z \mid \vec X, \vec \theta^{\text{old}}\right)$.
            \State M step: Hold $q(\vec Z)$ fixed and set $\vec \theta^{\text{new}} = \argmax_{\vec \theta} \left\{\mathcal Q\left(\vec \theta, \vec \theta^{\text{old}}\right) + \ln(\vec \theta)\right\}$. 
        \Until{convergence.}
    \end{algorithmic}
\end{algorithmbis}

\subsection{Gaussian mixture model}
Gaussian mixture model, a.k.a. GMM, or mixture of Gaussians is a mixture model where
\begin{align}
    p(z_n = k)                              &= \pi_k \\
    p(\vec x_n \mid z_n = k; \vec \theta)   &= \Gauss(\vec x_n \mid \vec \mu_k, \vec \Sigma_k)
\end{align}
for $n = 1, \dotsc, N$ and $k = 1, \dotsc, K$, where $\vec \theta = \left(\left\{\vec \mu_k, \vec \Sigma_k\right\}, k = 1, \dotsc, K\right)$.

\subsubsection{EM algorithm for GMM}
\begin{algorithmbis}[EM algorithm for GMM]\label{alg:models-mm-em-gmm}
    \begin{algorithmic}[1]
        \State Initialise $\vec \theta^{\text{new}} = \left(\left\{\vec \pi_k^{\text{new}}, \vec \mu_k^{\text{new}}, \vec \Sigma_k^{\text{new}}\right\}, k = 1, \dotsc, K\right)$.
        \Repeat
            \State $\vec \theta^{\text{old}} \gets \vec \theta^{\text{new}}$
            \State Set $r_{nk} = p\left(z_n = k \mid \vec x_n; \vec \theta^{\text{old}}\right)$ for $k = 1, \dotsc, K, n = 1, \dotsc, N$. \Comment E step
            \State Set \Comment M step
                \begin{align*}
                    \pi_k^{\text{new}}           &= \frac{\sum_n r_{nk}}{N} \\
                    \vec \mu_k^{\text{new}}      &= \frac{\sum_n r_{nk} \vec x_n}{\sum_n r_{nk}} \\
                    \vec \Sigma_k^{\text{new}}   &= \frac{\sum_n r_{nk}(\vec x_n - \vec \mu_k)(\vec x_n - \vec \mu_k)^T}{\sum_n r_{nk}}
                \end{align*}
                for $k = 1, \dotsc, K$.
        \Until{convergence.}
    \end{algorithmic}
\end{algorithmbis}

The analysis of the algorithm follows.
\paragraph{E step.} We can express $q(\vec Z = \vec K) = p(\vec Z = \vec K \mid \vec X, \vec \theta^{\text{old}})$ where $\vec K = (k_1, \dotsc, k_N), k_n \in \{1, \dotsc, K\}$ for $n =  1, \dotsc, N$ as
\begin{align*}
    p(\vec Z = \vec K \mid \vec X, \vec \theta^{\text{old}})    &= \prod_n p\left(z_n = k_n \mid \vec x_n; \vec \theta^{\text{old}}\right) \\
                                                                &= \prod_n r_{nk_n}\left(\vec \theta^{\text{old}}\right)
\end{align*}
Therefore, in the E step, we set
\begin{equation}
    r_{nk_n}\left(\vec \theta^{\text{old}}\right) = p\left(z_n = k_n \mid \vec x_n; \vec \theta^{\text{old}}\right)
\end{equation}
for $n = 1, \dotsc, N$ for all $\vec K$ and hold it fixed in the M step. This is effectively holding $r_{nk}\left(\vec \theta^{\text{old}}\right)$ (which we will abbreviate as $r_{nk}$ in this subsection) fixed for $n = 1, \dotsc, N$ and $k = 1, \dotsc, K$.

\paragraph{M step.} We want to find $\vec \theta^{\text{new}} = \argmax_{\vec \theta} \mathcal Q\left(\vec \theta, \vec \theta^{\text{old}}\right)$, where
\begin{align*}
    \mathcal Q\left(\vec \theta, \vec \theta^{\text{old}}\right)    &= \sum_n \sum_k r_{nk} \left(\ln \pi_k + \ln p(\vec x_n \mid z_n = k; \vec \theta)\right)
\end{align*}
To maximise this expression, we use Langrange multipliers because we have a constraint $\sum_k \pi_k = 1$. The Lagrangian is
\begin{equation*}
    \mathcal L_{\mathcal Q}(\vec \theta, \lambda) = \mathcal Q\left(\vec \theta, \vec \theta^{\text{old}}\right) + \lambda \left(1 - \sum_k \pi_k\right)
\end{equation*}
Now, we find the derivatives and set them to zero.

For $\pi_k$,
\begin{align*}
    \frac{\partial \mathcal L_{\mathcal Q}}{\partial \pi_k} &= \frac{\partial}{\partial \pi_k} \left\{\lambda \left(1 - \sum_j \pi_j\right) + \sum_n \sum_j r_{nj} \ln \pi_j\right\} \\
                                                            &= -\lambda + \frac{\sum_n r_{nk}}{\pi_k}
\end{align*}
Setting this to zero, we get
\begin{equation*}
    \pi_k = \frac{\sum_n r_{nk}}{\lambda}
\end{equation*}
but since $\sum_k \pi_k = 1$, we have $\sum_k \frac{\sum_n r_{nk}}{\lambda} = 1$, hence $\lambda = \sum_n \sum_k r_{nk} = \sum_n 1 = N$. Hence
\begin{equation}
    \pi_k = \frac{\sum_n r_{nk}}{N}
\end{equation}
for $k = 1, \dotsc, K$.

For $\vec \mu_k$,
\begin{align*}
    \grad_{\vec \mu_k} \mathcal L_{\mathcal Q}  &= \grad_{\vec \mu_k} \left\{\sum_n \sum_j r_{nj} \left(\ln \pi_j + \ln p(\vec x_n \mid z_n = j; \vec \theta)\right) + \lambda \left(1 - \sum_j \pi_j\right)\right\} \\
                                                &= \grad_{\vec \mu_k} \left\{\sum_n \sum_j r_{nj} \ln \Gauss\left(\vec x_n \mid \vec \mu_j, \vec \Sigma_j\right)\right\} \\
                                                &= \grad_{\vec \mu_k} \left\{\sum_n r_{nk} \ln \Gauss\left(\vec x_n \mid \vec \mu_k, \vec \Sigma_k\right)\right\} \\
                                                &= \grad_{\vec \mu_k} \left\{\sum_n r_{nk} \ln \left[(2\pi)^{-D / 2} |\vec \Sigma_k|^{-1 / 2} \exp{\left(-\frac{1}{2}(\vec x_n - \vec \mu_k)^T \vec \Sigma_k^{-1} (\vec x_n - \vec \mu_k)\right)}\right]\right\} \\
                                                &= \grad_{\vec \mu_k} \left\{\sum_n r_{nk} \left[-\frac{1}{2} \ln |\vec \Sigma_k| - \frac{1}{2}(\vec x_n - \vec \mu_k)^T \vec \Sigma_k^{-1} (\vec x_n - \vec \mu_k)\right]\right\} \\
                                                &= - \sum_n r_{nk} \vec \Sigma_k^{-1}(\vec x_n - \vec \mu_k)
\end{align*}
Setting this to zero, we get
\begin{equation}
    \vec \mu_k = \frac{\sum_n r_{nk} \vec x_n}{\sum_n r_{nk}}
\end{equation}
for $k = 1, \dotsc, K$.

For $\vec \Sigma_k$,
\begin{align*}
    \grad_{\vec \Sigma_k} \mathcal L_{\mathcal Q}   &= \grad_{\vec \Sigma_k} \left\{\sum_n r_{nk} \left[-\frac{1}{2} \ln |\vec \Sigma_k| - \frac{1}{2}(\vec x_n - \vec \mu_k)^T \vec \Sigma_k^{-1} (\vec x_n - \vec \mu_k)\right]\right\} \\
                                                    &= -\frac{1}{2} \sum_n r_{nk} \left[\vec \Sigma_k^{-T} - \vec \Sigma_k^{-T}(\vec x_n - \vec \mu_k)(\vec x_n - \vec \mu_k)^T \vec \Sigma_k^{-T}\right] \\
                                                    &= -\frac{1}{2} \vec \Sigma^{-1} \sum_n r_{nk} \left[\vec I - (\vec x_n - \vec \mu_k)(\vec x_n - \vec \mu_k)^T \vec \Sigma_k^{-1}\right]
\end{align*}
Setting this to zero, we get
\begin{equation}
    \vec \Sigma_k = \frac{\sum_n r_{nk}(\vec x_n - \vec \mu_k)(\vec x_n - \vec \mu_k)^T}{\sum_n r_{nk}}
\end{equation}